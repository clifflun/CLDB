import pandas as pd
import numpy as np
import sqlite3 as sq
from sqlalchemy import create_engine
import glob
import os

pd.set_option('display.expand_frame_repr', False)

# --- CONFIGURATION ---
# Path to the CGR files generated by CGR_cluster.py
CGR_OUTPUT_DIR = './data/3.0.0/CLDB_CGR_hg38_123125'
DB_NAME        = 'prod_CLDB_SR.sqlite'

def delete():
    print('Deleting old CGR_hg38 table...')
    conn = sq.connect(DB_NAME)
    cursor = conn.cursor()    
    cursor.execute('DROP TABLE IF EXISTS CGR_hg38')
    conn.commit()
    conn.close()
    print('Table deleted.')

def write_to_DB():
    print(f"Loading CGR Data from: {CGR_OUTPUT_DIR}")
    engine = create_engine(f'sqlite:///{DB_NAME}')
    
    # Pattern to match all patient files in the directory
    search_pattern = os.path.join(CGR_OUTPUT_DIR, "*.tsv")
    fnames = glob.glob(search_pattern)
    
    if not fnames:
        print(f"ERROR: No files found in {CGR_OUTPUT_DIR}")
        print("Please ensure CGR_cluster.py ran successfully and created this folder.")
        return

    print(f"  Found {len(fnames)} CGR files to load.")
    
    count = 0
    first_chunk = True
    
    # Iterate through all patient files and load them
    for fn in fnames:
        try:
            # Read each small file safely
            # Added low_memory=False and encoding='utf-8' to match P2/CNV scripts
            df = pd.read_csv(fn, sep='\t', low_memory=False, encoding='utf-8')
            
            if df.empty:
                continue

            if first_chunk:
                df.to_sql('CGR_hg38', con=engine, if_exists='replace', index=False)
                first_chunk = False
            else:
                df.to_sql('CGR_hg38', con=engine, if_exists='append', index=False)
            
            count += 1
            if count % 20 == 0:
                print(f"  Processed {count} files...")
                
        except Exception as e:
            print(f"Error loading {fn}: {e}")

    print(f"CGR Data loaded successfully ({count} files processed).")

def create_index():
    print("Creating indexes for CGR_hg38...")
    conn = sq.connect(DB_NAME)
    cursor = conn.cursor()
    
    # Indexes based on columns likely to be queried in CGR_cluster.py output
    # Note: 'chrom1' etc. come from the renamed columns in process_cnv
    _list = ['chrom1', 'pos1', 'pos2', 'SV_ID', 'SV_TYPE', 'PT_ID']
    
    for i in _list:
        try:
            print(f"  Creating index for column: {i}")
            cursor.execute(f'CREATE INDEX if NOT EXISTS {i}_index_cgr ON CGR_hg38 ({i})')
        except Exception as e:
            print(f"  Skipping index for {i} (column may not exist)")
            pass
            
    conn.commit()
    conn.close()
    print("Indexes created.")

def query():
    print("Verifying database content...")
    conn = sq.connect(DB_NAME)
    try:
        df = pd.read_sql("SELECT COUNT(*) as Count FROM CGR_hg38", conn)
        count = df.iloc[0]['Count']
        print(f"Total Rows in CGR_hg38: {count}")
        if count == 0:
             print("WARNING: Table is empty.")
    except Exception as e:
        print(f"Query failed: {e}")
    conn.close()

def main():
    print("--- Starting CGR Database Update ---")
    delete()
    write_to_DB()
    create_index()
    # Note: Metadata is handled by P2/CNV scripts, no need to reload here
    query()
    print("--- CGR Update Complete ---")

if __name__ == '__main__':
    main()