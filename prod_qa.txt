How many datasets are there?	This Variant Database operates under two separate datasets: [Carvalho Lab](https://pnri.org/carvalho-lab/) and [GREGoR Consortium](https://gregorconsortium.org/). Each dataset curates and manages distinct cohorts, catering to different research interests and clinical applications.
About GREGoR Consortium data.	1437 Illumina Short Read raw alignment data (CRAM) from each Reasearch Center were donwloaded from the U03 release on Terra. The DCC provides joint call dataset based on reprocessed data using the [WARP pipeline](https://github.com/broadinstitute/warp/tree/develop?tab=readme-ov-file). For more details of the dataset, please refer to the Datasets tab.
What are P2_SV, CNV, CGR, Sniffles2?	These are variant calls based on different tools. P2_SV are calls from [Parliament2](https://github.com/fritzsedlazeck/parliament2). CNV calls are read-depth based calls using a pipeline similar to [vizCNV](https://github.com/BCM-Lupskilab/VizCNV), a visualisation tool that we developed in collaboration with the Lupski Lab. CGR calls leverages P2_SV and CNV to search for Complex Genomic Rearrangements. [Sniffles2](https://github.com/fritzsedlazeck/Sniffles) is a SV caller for LR sequencing.
How does the P2_SV pipeline work?	All individuals are first processed using [Parliament2](https://github.com/fritzsedlazeck/parliament2). All of the SV calls are combined and clustered using DBSCAN. Each all is then annotated using publically available databases(gnomAD, OMIM, etc). Note: All SV calls with either breakpoint falling in *chr2:33141211-33141696 (hg19)/ chr2:32916144-32916629(hg38)* are removed due to technical artifacts inflating the number of calls present in the genome.
How does the CNV pipeline work?	[Mosdepth](https://github.com/brentp/mosdepth) was run on all individuals using a 10kb window. The read depth is normalized by genome median read depth. [SLMSeg](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-017-1734-5) was used to generate segments. Based on log2ratio of each segment, they are classified into different classes of CNVs. Similar to P2_SV, CNV calls are clustered using DBSCAN and annotated using public databases. Clusters are collapsed into one cluster if both breakpoints fall into their corresponding Inverted/Directed Repeats.
How does the CGR pipeline work?	For each individual, we started by identifying the closest SV for each CNV breakpoint. Then, we matched CNVs with corresponding SV types, coordinates, and sizes. Subsequently, we filtered the dataset to exclude CNVs with exact matching SVs and those overlapping segmental duplications (SDs) by at least 98%. Additionally, we applied a stringent proximity criterion, filtering CNVs with SVs located closer than 1.5kb from either breakpoint. 
How is pHaplo_Collins and pTrio_Collins annotated?	Data is downloaded from [UCSC](https://genome.ucsc.edu/cgi-bin/hgTables). Genes are filtered according to the cutoff provided on USCS. Then we look for genes that are within the filtered list of haploinsufficient/triplosensitive genes.
How is pHaplo_clinGen and pTrio_clinGen annotated?	Data is downloaded from [UCSC](https://genome.ucsc.edu/cgi-bin/hgTables). Genes are filtered according to the dosage score. The filtered list only contains regions with dosage score of 3, or 30, corresponding to sufficient evidence for dosage pathogenicity, and gene associated with autosomal recessive phenotype, respectively. 
Preprocessing of Segmental Duplication track.	Segmental duplication tracks are downloaded from USCS table browser. Overlapping regions are merged. Regions are also merged if the distance between regions is less than 1000bp. (bedtools merge -d 1000)
About variant clustering.	DBSCAN was used to cluster variants on a population level by left breakpoint, right breakpoint, and variant type. Cluster propensity is the number of variants that exist in a certain cluster. Due to the fact that multiple variants can come from the same subject, we have added the unique_id_count information for the calculation of the psuedo_df_freq. 